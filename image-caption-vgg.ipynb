{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2100424,"sourceType":"datasetVersion","datasetId":1259814}],"dockerImageVersionId":30554,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools\n\nfrom pycocotools.coco import COCO\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\n\nimport random\nimport string\n\nimport cv2\nimport os\nfrom pickle import dump, load\nimport json\n\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding, add\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tqdm.notebook import tqdm\npylab.rcParams['figure.figsize'] = (8.0, 10.0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dropout, LSTM\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coco = COCO(\"/kaggle/input/cocods/annotations_trainval2017/annotations/instances_train2017.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Finding Categories and Sub-Categories","metadata":{}},{"cell_type":"code","source":"cats = coco.loadCats(coco.getCatIds())\nmaincategories = list(set([cat['supercategory'] for cat in cats]))\n\nprint(\"Number of main categories: \", len(maincategories))\nprint(\"List of main categories: \", maincategories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subcategories = [cat['name'] for cat in cats]\n\nprint(\"Number of sub categories: \", len(subcategories))\nprint(\"List of sub categories: \", subcategories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catIds = coco.getCatIds(catNms=subcategories)\n\nsubcategories_Ids = dict()\nfor i in range(0,len(subcategories)):\n    subcategories_Ids[subcategories[i]] = catIds[i]\n\nprint(\"Sub categories with IDs :\",subcategories_Ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subcategories_imageIds = dict()\n\nfor i in range(0,len(catIds)):\n    imgIds = coco.getImgIds(catIds=catIds[i])\n    img = []\n    for j in imgIds: \n        img.append(j)\n    subcategories_imageIds[subcategories[i]] = img\n\nlength_dict = {key: len(value) for key, value in subcategories_imageIds.items()}\nprint(\"Total images in each sub categories: \", length_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only Bicycle Images have been considered due to computational limitations.","metadata":{}},{"cell_type":"code","source":"train_cats = subcategories_imageIds['bicycle'] \nimgIdss = coco.getImgIds(imgIds = train_cats)\nprint(\"Total Images: \", len(imgIdss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches(9, 9)\n\nnext_pix = imgIdss\nrandom.shuffle(next_pix)\n\nfor i, img_path in enumerate(next_pix[0:9]):\n    \n    sp = plt.subplot(3, 3, i + 1)\n    sp.axis('Off')\n\n    img = coco.loadImgs(img_path)[0]\n    I = io.imread(img['coco_url'])\n    plt.imshow(I)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annFile = \"../input/cocods/annotations_trainval2017/annotations/captions_train2017.json\"\ncoco_caps=COCO(annFile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = coco.loadImgs(next_pix[2])[0]\nI = io.imread(img['coco_url'])\nplt.imshow(I)\nannIds = coco_caps.getAnnIds(imgIds=img['id']);\nanns = coco_caps.loadAnns(annIds)\ncoco_caps.showAnns(anns)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preparing Dataset","metadata":{}},{"cell_type":"code","source":"dataset = dict()\nimgcaptions = []\n\nfor imgid in imgIdss:\n    img = coco.loadImgs(imgid)[0]\n    annIds = coco_caps.getAnnIds(imgIds=img['id']);\n    anns = coco_caps.loadAnns(annIds)\n    imgcaptions = []\n    for cap in anns:\n        \n        cap = cap['caption'].translate(str.maketrans('', '', string.punctuation))\n        \n        cap = cap.replace(\"-\",\" \")\n        \n        cap = cap.split()\n        cap = [word.lower() for word in cap]\n        \n        cap = '<start> ' + \" \".join(cap) + ' <end>'\n        imgcaptions.append(cap)\n        \n    dataset[img['coco_url']] = imgcaptions \n    \n    \nprint(\"Length of Dataset: \",len(dataset))\nprint(dataset['http://images.cocodataset.org/train2017/000000047084.jpg'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenizing Captions","metadata":{}},{"cell_type":"code","source":"from itertools import chain\nflatten_list = list(chain.from_iterable(dataset.values())) \n\ntokenizer = Tokenizer(oov_token='<oov>') \ntokenizer.fit_on_texts(flatten_list)\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary length: \", total_words)\nprint(\"Bicycle ID: \", tokenizer.word_index['bicycle'])\nprint(\"Airplane ID: \", tokenizer.word_index['airplane'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VGG16(include_top=True)\ntransfer_layer = model.get_layer('fc2')\nimage_model_transfer = Model(inputs=model.input,\n                             outputs=transfer_layer.output)\n\nimage_features = {}\n\nfor img in (dataset.keys()):\n    image = io.imread(img)\n    if image.ndim != 3:\n        image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n    \n    image = cv2.resize(image,(224,224))\n    image = np.expand_dims(image, axis=0)\n \n    image = image/255.0\n\n    feature = image_model_transfer.predict(image, verbose=0)\n    image_features[img] = feature\n    \nprint(\"Image features length: \", len(image_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_features['http://images.cocodataset.org/train2017/000000047084.jpg'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_to_list(descriptions):\n    all_desc = []\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\ndef max_length(descriptions):\n    desc_list = dict_to_list(descriptions)\n    return max(len(d.split()) for d in desc_list)\n    \nmax_length = max_length(dataset)\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_generator(descriptions, features, tokenizer, max_length):\n    while 1:\n        for key, description_list in descriptions.items():\n\n            feature = features[key][0]\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n            yield ([input_image, input_sequence], output_word)\n            \n\ndef create_sequences(tokenizer, max_length, desc_list, feature):\n    X1, X2, y = list(), list(), list()\n    \n    for desc in desc_list:\n        \n        seq = tokenizer.texts_to_sequences([desc])[0]\n        \n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            out_seq = to_categorical([out_seq], num_classes=total_words)[0]\n            \n            X1.append(feature) \n            X2.append(in_seq)  \n            y.append(out_seq)  \n            \n    return np.array(X1), np.array(X2), np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"def define_model(total_words, max_length):\n\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(total_words, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(total_words, activation='softmax')(decoder2)\n\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n    print(model.summary())\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Dataset: ', len(dataset))\nprint('Descriptions: train=', len(dataset))\nprint('Photos: train=', len(image_features))\nprint('Vocabulary Size:', total_words)\nprint('Description Length: ', max_length)\n\nmodel = define_model(total_words, max_length)\nepochs=1\nsteps = len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(epochs):\n    generator = data_generator(dataset, image_features, tokenizer, max_length)\n    model.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n    model.save(\"caption\" + str(i) + \".h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\nimg_paths = [\"../input/cocods/val2017/val2017/000000001761.jpg\",\n            \"../input/cocods/val2017/val2017/000000022396.jpg\",\n            \"../input/cocods/val2017/val2017/000000098520.jpg\",\n            \"../input/cocods/val2017/val2017/000000101762.jpg\",\n            \"../input/cocods/val2017/val2017/000000224051.jpg\"]\n\ndef extract_features(filename, model):\n        try:\n            image = Image.open(filename)\n\n        except:\n            print(\"ERROR: Couldn't open image!\")\n        image = image.resize((224,224))\n        image = np.array(image)\n        \n        if image.shape[2] == 4: \n            image = image[..., :3]\n        image = np.expand_dims(image, axis=0)\n        image = image/255.0\n        feature = model.predict(image)\n        return feature\n\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'start'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        pred = model.predict([photo,sequence], verbose=0)\n        pred = np.argmax(pred)\n        word = word_for_id(pred, tokenizer)\n        \n        if word is None:\n            break\n        in_text += ' ' + word\n        \n        if word == 'end':\n            break\n    return in_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_model = load_model('/kaggle/working/caption0.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photo = extract_features(img_paths[3], image_model_transfer)\nimg = Image.open(img_paths[3])\ndescription = generate_desc(pred_model, tokenizer, photo, 46)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","metadata":{},"execution_count":null,"outputs":[]}]}